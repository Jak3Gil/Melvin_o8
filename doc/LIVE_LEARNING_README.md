# Live Learning from Large Database

This tool pulls data from HTTP URLs continuously and shows live terminal stats plus real-time .m file outputs.

## Quick Start

### Basic Usage

```bash
# Pull from URL, save to SD card brain.m
./test_live_learning <url> /Volumes/512GB/brain.m --loop

# Or use the helper script
./start_live_learning.sh <url>
```

### Examples

```bash
# Example 1: Pull from a large dataset URL
./test_live_learning https://example.com/large-dataset.bin /Volumes/512GB/brain.m --loop

# Example 2: Custom chunk size (256KB chunks)
./test_live_learning https://example.com/dataset.bin /Volumes/512GB/brain.m --chunk-size 262144 --loop

# Example 3: Local brain file (for testing)
./test_live_learning https://example.com/dataset.bin brain.m --chunk-size 65536 --loop
```

## What You'll See

The terminal will show live updates every second with:

### ðŸ“Š Brain State
- **Nodes**: Total nodes in graph
- **Edges**: Total edges in graph
- **Adaptations**: How many times brain has been saved
- **Hierarchy**: Nodes by abstraction level (L0=raw, L1+=abstractions)

### ðŸ”Œ Port Activity
- **Port status**: Which ports are open/closed
- **Downloaded**: Bytes and chunks downloaded from HTTP
- **Written**: Bytes written to output file

### âš¡ Processing Stats
- **Elapsed time**: How long it's been running
- **Frames processed**: Number of chunks processed
- **Bytes downloaded**: Total MB downloaded
- **Rate**: Processing speed (frames/sec, MB/sec)

### ðŸ’­ Brain Output
- **Size**: Bytes of output generated
- **Hex**: Hexadecimal preview of output
- **Text**: ASCII preview of output (shows what brain is "thinking")

## Features

- **Memory Efficient**: Only one chunk in memory at a time (default 64KB-128KB)
- **Auto-Save**: Saves brain.m every 30 seconds automatically
- **Continuous**: Can loop through dataset indefinitely
- **Live Stats**: Real-time terminal updates showing everything
- **Output Preview**: See what the brain is generating in real-time

## Command Line Options

```
./test_live_learning <url> [brain.m] [--chunk-size SIZE] [--loop]
```

- `url`: HTTP/HTTPS URL to download (required)
- `brain.m`: Path to brain file (default: `brain.m`)
- `--chunk-size SIZE`: Size of each chunk in bytes (default: 65536 = 64KB)
- `--loop`: Loop back to start when EOF is reached (for continuous training)

## Chunk Size Guidelines

- **Small datasets (< 100MB)**: 32KB-64KB chunks
- **Medium datasets (100MB-1GB)**: 64KB-128KB chunks
- **Large datasets (1GB-10GB)**: 128KB-256KB chunks
- **Very large datasets (> 10GB)**: 256KB-512KB chunks

Larger chunks = less HTTP overhead but more memory per chunk.

## SD Card Setup

1. Mount SD card (should appear at `/Volumes/512GB`)
2. Run with SD card path:
   ```bash
   ./test_live_learning <url> /Volumes/512GB/brain.m --loop
   ```
3. Brain file will be created/updated on SD card
4. Output file will be saved as `/Volumes/512GB/brain.m.output`

## Output Files

- **brain.m**: The learning brain file (grows as it learns)
- **brain.m.output**: All outputs generated by the brain (append mode)

## Stopping

Press `Ctrl-C` to stop gracefully. The brain will:
1. Finish current chunk processing
2. Auto-save if there are changes
3. Show final statistics
4. Clean up and exit

## Troubleshooting

**"Could not open HTTP range request port"**
- Check URL is accessible
- Verify network connectivity
- Ensure server supports HTTP Range requests

**"SD card not found"**
- Check SD card is mounted
- Verify path: `ls /Volumes/512GB`
- Use local path for testing: `brain.m`

**Slow performance**
- Increase chunk size (reduces HTTP overhead)
- Check network speed
- Verify server performance

**No output shown**
- Normal for novel input (thinking mode)
- Output appears as patterns mature
- Check "Brain Output" section in stats

## Example URLs for Testing

You can test with any HTTP URL that supports Range requests:

- GitHub releases: `https://github.com/user/repo/releases/download/v1.0/file.bin`
- AWS S3: `https://bucket.s3.amazonaws.com/file.bin`
- Any CDN or web server with Range support

## Notes

- The brain learns continuously from the data stream
- Outputs are generated based on learned patterns
- Auto-save happens every 30 seconds (configurable in code)
- Memory usage stays constant regardless of dataset size
- Can run indefinitely with `--loop` flag

